# Test configuration for knowledge distillation
# Uses a smaller student model and requires a teacher checkpoint

defaults:
  - datasets: pickle
  - tasks: oc20_omol_conserving
  - backbone: K2L2
  - optimizer: adamw
  - _self_

job:
  device_type: CPU
  scheduler:
    mode: LOCAL
    distributed_init_method: FILE
  debug: True

# Distillation settings
teacher_checkpoint: ???  # Must be provided
distillation_coefficient: 1.0
ground_truth_coefficient: 1.0
distillation_loss_type: mse
distill_energy: True
distill_forces: True
distill_stress: False

# Model settings
act_type: s2
ff_type: grid
num_experts: 0
moe_layer_type: pytorch
expected_loss: null
direct_forces: False
regress_stress: True
max_neighbors: 20
checkpoint_every: null
max_steps: 3
max_epochs: null

# Student model heads (same structure as teacher for compatibility)
heads:
  energyandforcehead:
    module: fairchem.core.models.uma.escn_moe.DatasetSpecificMoEWrapper
    head_cls: fairchem.core.models.uma.escn_md.MLP_EFS_Head
    head_kwargs:
      wrap_property: False
    dataset_names:
      - omol
      - oc20

runner:
  _target_: fairchem.core.components.train.train_runner.TrainEvalRunner
  train_dataloader: ${datasets.train_dataloader}
  eval_dataloader: ${datasets.val_dataloader}
  train_eval_unit:
    _target_: fairchem.core.units.mlip_unit.distillation_unit.MLIPDistillationUnit
    job_config: ${job}
    tasks: ${tasks}
    teacher_checkpoint_location: ${teacher_checkpoint}
    distillation_coefficient: ${distillation_coefficient}
    ground_truth_coefficient: ${ground_truth_coefficient}
    distillation_loss_type: ${distillation_loss_type}
    distill_energy: ${distill_energy}
    distill_forces: ${distill_forces}
    distill_stress: ${distill_stress}
    teacher_overrides:
      backbone:
        max_neighbors: ${max_neighbors}
        otf_graph: True
    model:
      _target_: fairchem.core.units.mlip_unit.distillation_unit.initialize_student_model
      backbone_config: ${backbone}
      heads_config: ${heads}
      pass_through_head_outputs: True
    optimizer_fn: ${optimizer}
    cosine_lr_scheduler_fn:
      _target_: fairchem.core.units.mlip_unit.mlip_unit._get_consine_lr_scheduler
      _partial_: true
      warmup_factor: 0.2
      warmup_epochs: 0.01
      lr_min_factor: 0.01
      epochs: ${max_epochs}
      steps: ${max_steps}
    print_every: 1
    clip_grad_norm: 100
  max_epochs: ${max_epochs}
  max_steps: ${max_steps}
  evaluate_every_n_steps: 1000
  callbacks:
    - _target_: fairchem.core.components.train.train_runner.TrainCheckpointCallback
      checkpoint_every_n_steps: ${checkpoint_every}
      max_saved_checkpoints: 2
