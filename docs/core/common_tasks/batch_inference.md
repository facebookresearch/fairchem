# Batch inference

If you have many small systems that need to be evaluated or relaxed in parallel, it sometimes makes more sense to make model predictions as batches to improve GPU utilization. More details to be provided here soon! 