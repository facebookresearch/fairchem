{"version":3,"kind":"Notebook","sha256":"d26c9a80909a28ff5993a53fcf762aa8f8d796654c625fcdb0ce10227f0bc12a","slug":"fine-tuning","location":"/core/common_tasks/fine_tuning.md","dependencies":[],"frontmatter":{"title":"Fine-tuning","kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"},"jupytext":{"text_representation":{"extension":".md","format_name":"myst","format_version":"0.13","jupytext_version":"1.17.1"}},"content_includes_title":false,"authors":[{"nameParsed":{"literal":"FAIR Chemistry & Collaborators","given":"FAIR Chemistry &","family":"Collaborators"},"name":"FAIR Chemistry & Collaborators","id":"contributors-myst-generated-uid-0"}],"github":"https://github.com/facebookresearch/fairchem","copyright":"Meta Platforms, Inc","numbering":{"title":{"offset":2}},"source_url":"https://github.com/facebookresearch/fairchem/blob/main/docs/core/common_tasks/fine_tuning.md","edit_url":"https://github.com/facebookresearch/fairchem/edit/main/docs/core/common_tasks/fine_tuning.md","exports":[{"format":"md","filename":"fine_tuning.md","url":"/build/fine_tuning-b8a4e4ca0ff926ec81b1ff3595bab857.md"}]},"mdast":{"type":"root","children":[{"type":"block","children":[{"type":"paragraph","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"text","value":"This repo provides a number of scripts to quickly fine-tune a model using a custom ASE LMDB dataset. These scripts are merely for convenience and fine-tuning uses the exact same tooling and infrastructure as our standard training (see Training section). Training in the fairchem repo uses the fairchem CLI tool and configs are in ","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"Z4c6flv4TH"},{"type":"link","url":"https://hydra.cc/","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"text","value":"Hydra yaml","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"qLQV0AfZiQ"}],"urlSource":"https://hydra.cc/","key":"L6KN95NjT7"},{"type":"text","value":" format.","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"cGDf76TWTH"}],"key":"nAtg9oj0um"},{"type":"admonition","kind":"note","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"Note","key":"C7QVmlX5Yd"}],"key":"Pbu6BnIA31"},{"type":"paragraph","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"text","value":"Training datasets must be in the ","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"M6AsJf9lzn"},{"type":"link","url":"https://wiki.fysik.dtu.dk/ase/ase/db/db.html#ase.db.core.connect","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"text","value":"ASE-lmdb format","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"fcA4SE9OTx"}],"urlSource":"https://wiki.fysik.dtu.dk/ase/ase/db/db.html#ase.db.core.connect","key":"qqPEsykIfi"},{"type":"text","value":". For UMA models, we provide a simple script to help generate ASE-lmdb datasets from a variety of input formats (CIFs, traj, extxyz, etc.) as well as a fine-tuning YAML config that can be directly used for fine-tuning.","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"yx6mdZkLAa"}],"key":"kpNcsuIC0Q"}],"key":"MA2RE4wxRr"},{"type":"heading","depth":2,"position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"children":[{"type":"text","value":"Generating Training/Fine-tuning Datasets","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"key":"Ffw9uojbcx"}],"identifier":"generating-training-fine-tuning-datasets","label":"Generating Training/Fine-tuning Datasets","html_id":"generating-training-fine-tuning-datasets","implicit":true,"key":"PVdLhSrYyp"},{"type":"paragraph","position":{"start":{"line":24,"column":1},"end":{"line":24,"column":1}},"children":[{"type":"text","value":"First we need to generate a dataset in the aselmdb format for fine-tuning.","position":{"start":{"line":24,"column":1},"end":{"line":24,"column":1}},"key":"lnalGGL86k"}],"key":"gmbwzp8gpQ"},{"type":"admonition","kind":"tip","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"Tip","key":"NRUI6s0Fbz"}],"key":"THhnDsMSpR"},{"type":"paragraph","position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"children":[{"type":"text","value":"The only requirement is that you have input files that can be read as ASE atoms objects by the ","position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"key":"HaPTCyhMK2"},{"type":"inlineCode","value":"ase.io.read","position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"key":"eZwMsWu0V5"},{"type":"text","value":" routine and that they contain energy (forces, stress) in the correct format. For concrete examples, refer to the test at ","position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"key":"UpkRzS1Hw4"},{"type":"inlineCode","value":"tests/core/scripts/test_create_finetune_dataset.py","position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"key":"Z8N52pN3PC"},{"type":"text","value":".","position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"key":"yTxmU6naQi"}],"key":"sYbfEY1KPc"}],"key":"MdnFJymVwm"},{"type":"paragraph","position":{"start":{"line":30,"column":1},"end":{"line":30,"column":1}},"children":[{"type":"text","value":"First you should checkout the fairchem repo and install it to access the scripts","position":{"start":{"line":30,"column":1},"end":{"line":30,"column":1}},"key":"Mo6AQiiV5p"}],"key":"ZOiMsa9GNa"}],"key":"eNRFTNziQN"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"git clone git@github.com:facebookresearch/fairchem.git\n\npip install -e fairchem/src/packages/fairchem-core[dev]","visibility":"show","key":"NbzwbkuGJ8"},{"type":"outputs","id":"-zkAPEDs7egg28Vpj72hC","children":[],"visibility":"show","key":"oTf1A3PmRe"}],"data":{"tags":["skip-execution"]},"visibility":"show","key":"YtftXCSAfF"},{"type":"block","children":[{"type":"paragraph","position":{"start":{"line":39,"column":1},"end":{"line":39,"column":1}},"children":[{"type":"text","value":"Run this script to create the aselmdbs as well as a set of templated yamls for finetuning, we will use a few dummy structures for demonstration purposes","position":{"start":{"line":39,"column":1},"end":{"line":39,"column":1}},"key":"QqwdgXrpWO"}],"key":"dMOv39Xyxo"}],"key":"e9XLkMOgUs"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"import os\nos.chdir('../../../../fairchem')\n! python src/fairchem/core/scripts/create_uma_finetune_dataset.py --train-dir docs/core/common_tasks/finetune_assets/train/ --val-dir docs/core/common_tasks/finetune_assets/val --output-dir /tmp/bulk --uma-task=omat --regression-task e","key":"COHkb1OThe"},{"type":"outputs","id":"rJZYMbALP-5S4Qnr_m5ni","children":[{"type":"output","children":[],"jupyter_data":{"output_type":"stream","name":"stdout","text":"\r  0%|                                                     | 0/1 [00:00<?, ?it/s]\r\n\r\n\r0it [00:00, ?it/s]\u001b[A\u001b[A\r0it [00:00, ?it/s]\r\n"},"key":"fFMdPThSCQ"},{"type":"output","children":[],"jupyter_data":{"output_type":"stream","name":"stdout","text":"\r\n\r\n\r\n\r\n\r0it [00:00, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\r0it [00:00, ?it/s]\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r0it [00:00, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\r0it [00:00, ?it/s]\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r0it [00:00, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\r0it [00:00, ?it/s]\r\n\r\n\r  0%|                                                     | 0/1 [00:00<?, ?it/s]\u001b[A\r\n\r\n\r\n\r\n\r\n\r0it [00:00, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\r0it [00:00, ?it/s]\r\n\r\n\r\n\r\n\r0it [00:00, ?it/s]\u001b[A\u001b[A\u001b[A\r0it [00:00, ?it/s]\r\n\r100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 82.23it/s]\r\n\r100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 108.17it/s]\r\n"},"key":"VWjbDvmYwN"},{"type":"output","children":[],"jupyter_data":{"output_type":"stream","name":"stdout","text":"\rComputing normalizer values.:   0%|                       | 0/2 [00:00<?, ?it/s]\rComputing normalizer values.: 100%|██████████████| 2/2 [00:00<00:00, 792.95it/s]\r\n"},"key":"DilKnj8Sca"},{"type":"output","children":[],"jupyter_data":{"output_type":"stream","name":"stdout","text":"\r  0%|                                                     | 0/1 [00:00<?, ?it/s]\r\n\r  0%|                                                     | 0/1 [00:00<?, ?it/s]\u001b[A\r\n\r\n\r\n\r\n\r\n\r0it [00:00, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\r0it [00:00, ?it/s]\r\n\r\n\r\n\r\n\r0it [00:00, ?it/s]\u001b[A\u001b[A\u001b[A\r0it [00:00, ?it/s]\r\n\r\n\r\n\r0it [00:00, ?it/s]\u001b[A\u001b[A\r0it [00:00, ?it/s]\r\n\r\n\r\n\r\n\r\n\r0it [00:00, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\r0it [00:00, ?it/s]\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r0it [00:00, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\r0it [00:00, ?it/s]\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r0it [00:00, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\r0it [00:00, ?it/s]\r\n\r100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 91.72it/s]\r\n\r100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 84.27it/s]\r\n"},"key":"kv11bjgmQs"},{"type":"output","children":[],"jupyter_data":{"output_type":"stream","name":"stdout","text":"INFO:root:Generated dataset and data config yaml in /tmp/bulk\r\nINFO:root:To run finetuning, run fairchem -c /tmp/bulk/uma_sm_finetune_template.yaml\r\n"},"key":"j9LOxsHIfj"}],"key":"ZLkzP80JsU"}],"key":"FLwjd61dfg"},{"type":"block","children":[{"type":"admonition","kind":"warning","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"Warning","key":"DvropT7Zwu"}],"key":"nzR9pxbtvN"},{"type":"paragraph","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"children":[{"type":"strong","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"children":[{"type":"text","value":"Task Selection:","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"l0hKMsadK0"}],"key":"UZD9DvzyTs"},{"type":"text","value":" The ","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"JV23tDbns0"},{"type":"inlineCode","value":"uma-task","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"q5UAjTHu0p"},{"type":"text","value":" can be one of: ","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"ZHTLICuUbo"},{"type":"inlineCode","value":"omol","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"aaCBg7rybq"},{"type":"text","value":", ","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"o0BelqPKTz"},{"type":"inlineCode","value":"odac","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"moYzMYnUxS"},{"type":"text","value":", ","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"i7bHSPAg4h"},{"type":"inlineCode","value":"oc20","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"vkiajXScry"},{"type":"text","value":", ","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"BWyFTOA024"},{"type":"inlineCode","value":"omat","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"JeGwoky6U6"},{"type":"text","value":", ","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"xeNicLbm0k"},{"type":"inlineCode","value":"omc","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"OHh3SblP32"},{"type":"text","value":". While UMA was trained in a multi-task fashion, we ONLY support fine-tuning on a single UMA task at a time. Multi-task training can become very complicated! Feel free to contact us on GitHub if you have a special use-case for multi-task fine-tuning, or refer to the training configs in ","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"YmT0f9I3CN"},{"type":"inlineCode","value":"/training_release","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"tWsF2SCF3f"},{"type":"text","value":" to mimic the original UMA training configs.","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"uLdw1fJ1FH"}],"key":"J5oUqNUB0K"}],"key":"Ko1kbKLyTZ"},{"type":"admonition","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"Regression Task Options","position":{"start":{"line":50,"column":1},"end":{"line":50,"column":1}},"key":"nTbGStD51Z"}],"key":"uJ6q70QnEk"},{"type":"paragraph","position":{"start":{"line":53,"column":1},"end":{"line":53,"column":1}},"children":[{"type":"text","value":"The ","position":{"start":{"line":53,"column":1},"end":{"line":53,"column":1}},"key":"EBuXUFgBYC"},{"type":"inlineCode","value":"regression-task","position":{"start":{"line":53,"column":1},"end":{"line":53,"column":1}},"key":"qAvoUV65bo"},{"type":"text","value":" can be one of:","position":{"start":{"line":53,"column":1},"end":{"line":53,"column":1}},"key":"VXXNjQ0PO8"}],"key":"Y2vmb0qUFM"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":54,"column":1},"end":{"line":57,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":54,"column":1},"end":{"line":54,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":54,"column":1},"end":{"line":54,"column":1}},"children":[{"type":"text","value":"e","position":{"start":{"line":54,"column":1},"end":{"line":54,"column":1}},"key":"gfFSsj1McZ"}],"key":"b4wV60QFcF"},{"type":"text","value":": Energy only","position":{"start":{"line":54,"column":1},"end":{"line":54,"column":1}},"key":"gmcblXDnNq"}],"key":"HtPuFpturo"}],"key":"b2SydLXzZL"},{"type":"listItem","spread":true,"position":{"start":{"line":55,"column":1},"end":{"line":55,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":55,"column":1},"end":{"line":55,"column":1}},"children":[{"type":"text","value":"ef","position":{"start":{"line":55,"column":1},"end":{"line":55,"column":1}},"key":"OvVg5JU7ht"}],"key":"gmBy9UsTrf"},{"type":"text","value":": Energy + forces","position":{"start":{"line":55,"column":1},"end":{"line":55,"column":1}},"key":"VRat1WhNct"}],"key":"UUx01DPL4d"}],"key":"t1iEfWJHHi"},{"type":"listItem","spread":true,"position":{"start":{"line":56,"column":1},"end":{"line":57,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":56,"column":1},"end":{"line":56,"column":1}},"children":[{"type":"text","value":"efs","position":{"start":{"line":56,"column":1},"end":{"line":56,"column":1}},"key":"uwLrZjP06K"}],"key":"VqXhQgH6W0"},{"type":"text","value":": Energy + forces + stress","position":{"start":{"line":56,"column":1},"end":{"line":56,"column":1}},"key":"Dvoynx8Okj"}],"key":"u6ajl1SzOB"}],"key":"uHSLYPafAl"}],"key":"RYx5HspEM1"},{"type":"paragraph","position":{"start":{"line":58,"column":1},"end":{"line":58,"column":1}},"children":[{"type":"text","value":"Choose based on the data you have available in the ASE db. For example, some aperiodic DFT codes only support energy/forces and not gradients, and some very fancy codes like QMC only produce energies.","position":{"start":{"line":58,"column":1},"end":{"line":58,"column":1}},"key":"Xzdc9XBF46"}],"key":"kIK3dsU1RP"},{"type":"paragraph","position":{"start":{"line":60,"column":1},"end":{"line":60,"column":1}},"children":[{"type":"strong","position":{"start":{"line":60,"column":1},"end":{"line":60,"column":1}},"children":[{"type":"text","value":"Note:","position":{"start":{"line":60,"column":1},"end":{"line":60,"column":1}},"key":"zMe6gngode"}],"key":"NloHH4sH9I"},{"type":"text","value":" Even if you train on just energy or energy/forces, all gradients (forces/stresses) will be computable via the model gradients.","position":{"start":{"line":60,"column":1},"end":{"line":60,"column":1}},"key":"rS7AxmPC7R"}],"key":"xyfS0SHVgn"}],"class":"dropdown","key":"l6sXSXY9Aj"},{"type":"paragraph","position":{"start":{"line":63,"column":1},"end":{"line":63,"column":1}},"children":[{"type":"text","value":"This will generate a folder of LMDBs and a ","position":{"start":{"line":63,"column":1},"end":{"line":63,"column":1}},"key":"mWMuDtwWjY"},{"type":"inlineCode","value":"uma_sm_finetune_template.yaml","position":{"start":{"line":63,"column":1},"end":{"line":63,"column":1}},"key":"cDNCXJliLB"},{"type":"text","value":" that you can run directly with the fairchem CLI to start training.","position":{"start":{"line":63,"column":1},"end":{"line":63,"column":1}},"key":"xk9OrQ2C21"}],"key":"VgLntNtsqO"},{"type":"admonition","kind":"tip","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"Tip","key":"SYkTXtXlUu"}],"key":"rDI1tdRYas"},{"type":"paragraph","position":{"start":{"line":66,"column":1},"end":{"line":66,"column":1}},"children":[{"type":"text","value":"If you want to only create the ASE LMDBs, you can use ","position":{"start":{"line":66,"column":1},"end":{"line":66,"column":1}},"key":"AQ8403sRF9"},{"type":"inlineCode","value":"src/fairchem/core/scripts/create_finetune_dataset.py","position":{"start":{"line":66,"column":1},"end":{"line":66,"column":1}},"key":"Pz5THRJNDM"},{"type":"text","value":" which is called by ","position":{"start":{"line":66,"column":1},"end":{"line":66,"column":1}},"key":"NLj7HOEQ12"},{"type":"inlineCode","value":"create_uma_finetune_dataset.py","position":{"start":{"line":66,"column":1},"end":{"line":66,"column":1}},"key":"I0DawOY9zy"},{"type":"text","value":".","position":{"start":{"line":66,"column":1},"end":{"line":66,"column":1}},"key":"PdNlXIXAR7"}],"key":"ikNQi4mMem"}],"key":"OS9c3WSg1w"},{"type":"heading","depth":2,"position":{"start":{"line":69,"column":1},"end":{"line":69,"column":1}},"children":[{"type":"text","value":"Model Fine-tuning (Default Settings)","position":{"start":{"line":69,"column":1},"end":{"line":69,"column":1}},"key":"kjF80heHZ3"}],"identifier":"model-fine-tuning-default-settings","label":"Model Fine-tuning (Default Settings)","html_id":"model-fine-tuning-default-settings","implicit":true,"key":"gOtjjiy2KZ"},{"type":"paragraph","position":{"start":{"line":71,"column":1},"end":{"line":71,"column":1}},"children":[{"type":"text","value":"The previous step should have generated some YAML files to get you started on fine-tuning. You can simply run this with the ","position":{"start":{"line":71,"column":1},"end":{"line":71,"column":1}},"key":"Xd54bu7g8t"},{"type":"inlineCode","value":"fairchem","position":{"start":{"line":71,"column":1},"end":{"line":71,"column":1}},"key":"zYuN1DR8NG"},{"type":"text","value":" CLI. The default is configured to run locally on 1 GPU.","position":{"start":{"line":71,"column":1},"end":{"line":71,"column":1}},"key":"U0l47jacbp"}],"key":"YhCVuRuyB0"}],"key":"ERDeEMK4cL"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"! fairchem -c /tmp/bulk/uma_sm_finetune_template.yaml","visibility":"show","key":"lcesaIHKIK"},{"type":"outputs","id":"rXHeo8YVv-5dlB3gYhTJh","children":[],"visibility":"show","key":"gXOQcLL4mf"}],"data":{"tags":["skip-execution"]},"visibility":"show","key":"vb3mY03hw4"},{"type":"block","children":[{"type":"heading","depth":2,"position":{"start":{"line":78,"column":1},"end":{"line":78,"column":1}},"children":[{"type":"text","value":"Advanced Configuration","position":{"start":{"line":78,"column":1},"end":{"line":78,"column":1}},"key":"E9NQZLm2Kk"}],"identifier":"advanced-configuration","label":"Advanced Configuration","html_id":"advanced-configuration","implicit":true,"key":"qKirTplbdX"},{"type":"paragraph","position":{"start":{"line":80,"column":1},"end":{"line":80,"column":1}},"children":[{"type":"text","value":"The scripts provide a simple way to get started on fine-tuning, but likely for your own use cases you will need to modify the parameters. The configuration uses ","position":{"start":{"line":80,"column":1},"end":{"line":80,"column":1}},"key":"aXy0imdTwY"},{"type":"link","url":"https://hydra.cc/","position":{"start":{"line":80,"column":1},"end":{"line":80,"column":1}},"children":[{"type":"text","value":"Hydra-style YAMLs","position":{"start":{"line":80,"column":1},"end":{"line":80,"column":1}},"key":"cyywcOJzp8"}],"urlSource":"https://hydra.cc/","key":"qmBXj1fb6x"},{"type":"text","value":".","position":{"start":{"line":80,"column":1},"end":{"line":80,"column":1}},"key":"RKf1q8szcx"}],"key":"BFlRMSXqrb"},{"type":"admonition","kind":"tip","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"Tip","key":"UpqXJlOICW"}],"key":"MazrFQdEyy"},{"type":"paragraph","position":{"start":{"line":83,"column":1},"end":{"line":83,"column":1}},"children":[{"type":"text","value":"To modify the generated YAMLs, you can either edit the files directly or use ","position":{"start":{"line":83,"column":1},"end":{"line":83,"column":1}},"key":"frsZk7EBmi"},{"type":"link","url":"https://hydra.cc/docs/advanced/override_grammar/basic/","position":{"start":{"line":83,"column":1},"end":{"line":83,"column":1}},"children":[{"type":"text","value":"Hydra override notation","position":{"start":{"line":83,"column":1},"end":{"line":83,"column":1}},"key":"z6pDIyFWdG"}],"urlSource":"https://hydra.cc/docs/advanced/override_grammar/basic/","key":"NBvOoz0jn9"},{"type":"text","value":". Changing parameters on the command line is very simple:","position":{"start":{"line":83,"column":1},"end":{"line":83,"column":1}},"key":"DquGJCSSZa"}],"key":"gjghTnPzEt"}],"key":"TuluiCgSrW"}],"key":"ZWzgBUuVT4"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"! fairchem -c /tmp/bulk/uma_sm_finetune_template.yaml epochs=2 lr=2e-4 job.run_dir=/tmp/finetune_dir +job.timestamp_id=some_id","key":"Mz7IWrYia5"},{"type":"outputs","id":"MIitYGO4C3CMtsKpbVEqV","children":[{"type":"output","children":[],"jupyter_data":{"output_type":"stream","name":"stdout","text":"INFO:root:saved canonical config to /tmp/finetune_dir/some_id/canonical_config.yaml\r\nINFO:root:Running fairchemv2 cli with {'job': {'run_name': 'uma_finetune', 'timestamp_id': 'some_id', 'run_dir': '/tmp/finetune_dir', 'device_type': <DeviceType.CUDA: 'cuda'>, 'debug': True, 'scheduler': {'mode': <SchedulerType.LOCAL: 'local'>, 'distributed_init_method': <DistributedInitMethod.TCP: 'tcp'>, 'ranks_per_node': 1, 'num_nodes': 1, 'num_array_jobs': 1, 'slurm': {'mem_gb': 80, 'timeout_hr': 168, 'cpus_per_task': 8, 'partition': None, 'qos': None, 'account': None, 'additional_parameters': None}, 'use_ray': False, 'ray_cluster': {'head_gpus': 0}}, 'logger': {'_target_': 'fairchem.core.common.logger.WandBSingletonLogger.init_wandb', '_partial_': True, 'entity': 'example', 'project': 'uma_finetune'}, 'seed': 0, 'deterministic': False, 'runner_state_path': None, 'metadata': {'commit': 'core:None,experimental:NA', 'log_dir': '/tmp/finetune_dir/some_id/logs', 'checkpoint_dir': '/tmp/finetune_dir/some_id/checkpoints', 'results_dir': '/tmp/finetune_dir/some_id/results', 'config_path': '/tmp/finetune_dir/some_id/canonical_config.yaml', 'preemption_checkpoint_dir': '/tmp/finetune_dir/some_id/checkpoints/preemption_state', 'cluster_name': 'github', 'array_job_num': 0, 'slurm_env': {'job_id': None, 'raw_job_id': None, 'array_job_id': None, 'array_task_id': None, 'restart_count': None}}, 'graph_parallel_group_size': None, 'recursive_instantiate_runner': True}, 'runner': {'_target_': 'fairchem.core.components.train.train_runner.TrainEvalRunner', 'train_dataloader': {'_target_': 'fairchem.core.components.common.dataloader_builder.get_dataloader', 'dataset': {'_target_': 'fairchem.core.datasets.mt_concat_dataset.create_concat_dataset', 'dataset_configs': {'omat': {'splits': {'train': {'src': '/tmp/bulk/train'}}, 'format': 'ase_db', 'transforms': {'common_transform': {'dataset_name': 'omat'}, 'stress_reshape_transform': {'dataset_name': 'omat'}}}}, 'combined_dataset_config': {'sampling': {'"},"key":"uFNMM82UI8"},{"type":"output","children":[],"jupyter_data":{"output_type":"stream","name":"stdout","text":"type': 'temperature', 'temperature': 1.0}}}, 'batch_sampler_fn': {'_target_': 'fairchem.core.common.data_parallel.BalancedBatchSampler', '_partial_': True, 'batch_size': 2, 'shuffle': True, 'seed': 0}, 'num_workers': 0, 'collate_fn': {'_target_': 'fairchem.core.units.mlip_unit.mlip_unit.mt_collater_adapter', 'tasks': [{'_target_': 'fairchem.core.units.mlip_unit.mlip_unit.Task', 'name': 'energy', 'level': 'system', 'property': 'energy', 'loss_fn': {'_target_': 'fairchem.core.modules.loss.DDPMTLoss', 'loss_fn': {'_target_': 'fairchem.core.modules.loss.PerAtomMAELoss'}, 'coefficient': 20}, 'out_spec': {'dim': [1], 'dtype': 'float32'}, 'normalizer': {'_target_': 'fairchem.core.modules.normalization.normalizer.Normalizer', 'mean': 0.0, 'rmsd': 1.0}, 'element_references': {'_target_': 'fairchem.core.modules.normalization.element_references.ElementReferences', 'element_references': {'_target_': 'torch.DoubleTensor', '_args_': [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.082805460035017, 0.0, 0.0, 0.0, -3.256404920200298, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]}}, 'datasets': ['omat'], 'metrics': ['mae', 'per_atom_mae']}, {'_target_': 'fairchem.core.units.mlip_unit.mlip_unit.Task', 'name': 'forces', 'level': 'atom', 'property': 'forces', 'out_spec': {'dim': [3], 'dtype': 'float32'}, 'normalizer': {'_target_': 'fairchem.core.modules.normalization.normalizer.Normalizer', 'mean': 0.0, 'rmsd': 1.0}, 'datasets': ['omat'], 'inference_only': True}, {'_target_': 'fairchem.core.units.mlip_unit.mlip_unit.Task', 'name': 'stress', 'level': 'system', 'property': 'stress', 'out_spec': {'dim': [1, 9], 'dtype': 'float32'}, 'normalizer': {'_target_': 'fairchem.core.modules.normalization.normalizer.Normalizer', 'mean': 0.0, 'rmsd': 1.0}, 'datasets': ['omat'], 'inference_only': True}]}}, 'eval_dataloader': {'_target_': 'fairchem.core.components.common.dataloader_builder.get_dataloader', 'dataset': {'_target_': 'fairchem.core.datasets.mt_concat_dataset.create_concat_dataset', 'dataset_configs': {'omat': {'splits': {'val': {'src': '/tmp/bulk/val'}}, 'format': 'ase_db', 'transforms': {'common_transform': {'dataset_name': 'omat'}, 'stress_reshape_transform': {'dataset_name': 'omat'}}}}, 'combined_dataset_config': {'sampling': {'type': 'temperature', 'temperature': 1.0}}}, 'batch_sampler_fn': {'_target_': 'fairchem.core.common.data_parallel.BalancedBatchSampler', '_partial_': True, 'batch_size': 2, 'shuffle': False, 'seed': 0}, 'num_workers': 0, 'collate_fn': {'_target_': 'fairchem.core.units.mlip_unit.mlip_unit.mt_collater_adapter', 'tasks': [{'_target_': 'fairchem.core.units.mlip_unit.mlip_unit.Task', 'name': 'energy', 'level': 'system', 'property': 'energy', 'loss_fn': {'_target_': 'fairchem.core.modules.loss.DDPMTLoss', 'loss_fn': {'_target_': 'fairchem.core.modules.loss.PerAtomMAELoss'}, 'coefficient': 20}, 'out_spec': {'dim': [1], 'dtype': 'float32'}, 'normalizer': {'_target_': 'fairchem.core.modules.normalization.normalizer.Normalizer', 'mean': 0.0, 'rmsd': 1.0}, 'element_references': {'_target_': 'fairchem.core.modules.normalization.element_references.ElementReferences', 'element_references': {'_target_': 'torch.DoubleTensor', '_args_': [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.082805460035017, 0.0, 0.0, 0.0, -3.256404920200298, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]}}, 'datasets': ['omat'], 'metrics': ['mae', 'per_atom_mae']}, {'_target_': 'fairchem.core.units.mlip_unit.mlip_unit.Task', 'name': 'forces', 'level': 'atom', 'property': 'forces', 'out_spec': {'dim': [3], 'dtype': 'float32'}, 'normalizer': {'_target_': 'fairchem.core.modules.normalization.normalizer.Normalizer', 'mean': 0.0, 'rmsd': 1.0}, 'datasets': ['omat'], 'inference_only': True}, {'_target_': 'fairchem.core.units.mlip_unit.mlip_unit.Task', 'name': 'stress', 'level': 'system', 'property': 'stress', 'out_spec': {'dim': [1, 9], 'dtype': 'float32'}, 'normalizer': {'_target_': 'fairchem.core.modules.normalization.normalizer.Normalizer', 'mean': 0.0, 'rmsd': 1.0}, 'datasets': ['omat'], 'inference_only': True}]}}, 'train_eval_unit': {'_target_': 'fairchem.core.units.mlip_unit.mlip_unit.MLIPTrainEvalUnit', 'job_config': {'run_name': 'uma_finetune', 'timestamp_id': 'some_id', 'run_dir': '/tmp/finetune_dir', 'device_type': <DeviceType.CUDA: 'cuda'>, 'debug': True, 'scheduler': {'mode': <SchedulerType.LOCAL: 'local'>, 'distributed_init_method': <DistributedInitMethod.TCP: 'tcp'>, 'ranks_per_node': 1, 'num_nodes': 1, 'num_array_jobs': 1, 'slurm': {'mem_gb': 80, 'timeout_hr': 168, 'cpus_per_task': 8, 'partition': None, 'qos': None, 'account': None, 'additional_parameters': None}, 'use_ray': False, 'ray_cluster': {'head_gpus': 0}}, 'logger': {'_target_': 'fairchem.core.common.logger.WandBSingletonLogger.init_wandb', '_partial_': True, 'entity': 'example', 'project': 'uma_finetune'}, 'seed': 0, 'deterministic': False, 'runner_state_path': None, 'metadata': {'commit': 'core:None,experimental:NA', 'log_dir': '/tmp/finetune_dir/some_id/logs', 'checkpoint_dir': '/tmp/finetune_dir/some_id/checkpoints', 'results_dir': '/tmp/finetune_dir/some_id/results', 'config_path': '/tmp/finetune_dir/some_id/canonical_config.yaml', 'preemption_checkpoint_dir': '/tmp/finetune_dir/some_id/checkpoints/preemption_state', 'cluster_name': 'github', 'array_job_num': 0, 'slurm_env': {'job_id': None, 'raw_job_id': None, 'array_job_id': None, 'array_task_id': None, 'restart_count': None}}, 'graph_parallel_group_size': None, 'recursive_instantiate_runner': True}, 'tasks': [{'_target_': 'fairchem.core.units.mlip_unit.mlip_unit.Task', 'name': 'energy', 'level': 'system', 'property': 'energy', 'loss_fn': {'_target_': 'fairchem.core.modules.loss.DDPMTLoss', 'loss_fn': {'_target_': 'fairchem.core.modules.loss.PerAtomMAELoss'}, 'coefficient': 20}, 'out_spec': {'dim': [1], 'dtype': 'float32'}, 'normalizer': {'_target_': 'fairchem.core.modules.normalization.normalizer.Normalizer', 'mean': 0.0, 'rmsd': 1.0}, 'element_references': {'_target_': 'fairchem.core.modules.normalization.element_references.ElementReferences', 'element_references': {'_target_': 'torch.DoubleTensor', '_args_': [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.082805460035017, 0.0, 0.0, 0.0, -3.256404920200298, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]}}, 'datasets': ['omat'], 'metrics': ['mae', 'per_atom_mae']}, {'_target_': 'fairchem.core.units.mlip_unit.mlip_unit.Task', 'name': 'forces', 'level': 'atom', 'property': 'forces', 'out_spec': {'dim': [3], 'dtype': 'float32'}, 'normalizer': {'_target_': 'fairchem.core.modules.normalization.normalizer.Normalizer', 'mean': 0.0, 'rmsd': 1.0}, 'datasets': ['omat'], 'inference_only': True}, {'_target_': 'fairchem.core.units.mlip_unit.mlip_unit.Task', 'name': 'stress', 'level': 'system', 'property': 'stress', 'out_spec': {'dim': [1, 9], 'dtype': 'float32'}, 'normalizer': {'_target_': 'fairchem.core.modules.normalization.normalizer.Normalizer', 'mean': 0.0, 'rmsd': 1.0}, 'datasets': ['omat'], 'inference_only': True}], 'model': {'_target_': 'fairchem.core.units.mlip_unit.mlip_unit.initialize_finetuning_model', 'checkpoint_location': {'_target_': 'fairchem.core.calculate.pretrained_mlip.pretrained_checkpoint_path_from_name', 'model_name': 'uma-s-1'}, 'overrides': {'backbone': {'otf_graph': True, 'max_neighbors': 300, 'regress_stress': True, 'always_use_pbc': False}, 'pass_through_head_outputs': True}, 'heads': {'efs': {'module': 'fairchem.core.models.uma.escn_md.MLP_EFS_Head'}}}, 'optimizer_fn': {'_target_': 'torch.optim.AdamW', '_partial_': True, 'lr': 0.0002, 'weight_decay': 0.001}, 'cosine_lr_scheduler_fn': {'_target_': 'fairchem.core.units.mlip_unit.mlip_unit._get_consine_lr_scheduler', '_partial_': True, 'warmup_factor': 0.2, 'warmup_epochs': 0.01, 'lr_min_factor': 0.01, 'epochs': 2, 'steps': None}, 'print_every': 10, 'clip_grad_norm': 100}, 'max_epochs': 2, 'max_steps': None, 'evaluate_every_n_steps': 100, 'callbacks': [{'_target_': 'fairchem.core.components.train.train_runner.TrainCheckpointCallback', 'checkpoint_every_n_steps': 1000, 'max_saved_checkpoints': 5}, {'_target_': 'torchtnt.framework.callbacks.TQDMProgressBar'}]}}\r\n"},"key":"kVXTEBT3Kl"},{"type":"output","children":[],"jupyter_data":{"output_type":"stream","name":"stdout","text":"INFO:root:Running in local mode with 1 ranks using device_type:cuda\r\nINFO:root:Running in local mode without elastic launch\r\nINFO:root:Setting env PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\r\nINFO:root:Setting up distributed backend...\r\n"},"key":"ISHwsek7EB"},{"type":"output","children":[],"jupyter_data":{"output_type":"stream","name":"stdout","text":"INFO:root:Calling runner.run() ...\r\n"},"key":"hBB7Xrl9M4"},{"type":"output","children":[],"jupyter_data":{"output_type":"stream","name":"stdout","text":"INFO:root:get_dataloader::Calling batch_sampler_fn=functools.partial(<class 'fairchem.core.common.data_parallel.BalancedBatchSampler'>, batch_size=2, shuffle=True, seed=0)...\r\nWARNING:root:Disabled BalancedBatchSampler because num_replicas=1.\r\nINFO:root:rank: 0: Sampler created...\r\nINFO:root:Created BalancedBatchSampler with sampler=<fairchem.core.common.data_parallel.StatefulDistributedSampler object at 0x7981c775dd60>, batch_size=2, drop_last=False\r\nINFO:root:get_dataloader::Calling Dataloader...\r\nINFO:root:get_dataloader::Done!\r\nINFO:root:get_dataloader::Calling batch_sampler_fn=functools.partial(<class 'fairchem.core.common.data_parallel.BalancedBatchSampler'>, batch_size=2, shuffle=False, seed=0)...\r\nWARNING:root:Disabled BalancedBatchSampler because num_replicas=1.\r\nINFO:root:rank: 0: Sampler created...\r\nINFO:root:Created BalancedBatchSampler with sampler=<fairchem.core.common.data_parallel.StatefulDistributedSampler object at 0x7981c76ec6e0>, batch_size=2, drop_last=False\r\nINFO:root:get_dataloader::Calling Dataloader...\r\nINFO:root:get_dataloader::Done!\r\n"},"key":"z9tXb5JAxC"},{"type":"output","children":[],"jupyter_data":{"output_type":"stream","name":"stdout","text":"INFO:httpx:HTTP Request: HEAD https://huggingface.co/facebook/UMA/resolve/main/checkpoints/uma-s-1.pt \"HTTP/1.1 302 Found\"\r\n"},"key":"MJVPhlFlrv"},{"type":"output","children":[],"jupyter_data":{"output_type":"stream","name":"stdout","text":"INFO:httpx:HTTP Request: GET https://huggingface.co/api/models/facebook/UMA/xet-read-token/38529caa2c51a9a8a0d71f0b56b79ac33bc9eceb \"HTTP/1.1 200 OK\"\r\n\rcheckpoints/uma-s-1.pt:   0%|                       | 0.00/1.17G [00:00<?, ?B/s]"},"key":"Oi4EskTGHa"},{"type":"output","children":[],"jupyter_data":{"output_type":"stream","name":"stdout","text":"\rcheckpoints/uma-s-1.pt:   0%|                | 615k/1.17G [00:01<34:36, 565kB/s]"},"key":"CQJwnFKH3Q"},{"type":"output","children":[],"jupyter_data":{"output_type":"stream","name":"stdout","text":"\rcheckpoints/uma-s-1.pt:   3%|▍             | 34.2M/1.17G [00:01<00:52, 21.8MB/s]"},"key":"YwWCfiLP2z"},{"type":"output","children":[],"jupyter_data":{"output_type":"stream","name":"stdout","text":"\rcheckpoints/uma-s-1.pt:   9%|█▎             | 101M/1.17G [00:02<00:14, 74.7MB/s]"},"key":"tlwEDVQFnV"},{"type":"output","children":[],"jupyter_data":{"output_type":"stream","name":"stdout","text":"\rcheckpoints/uma-s-1.pt:  14%|██▎             | 168M/1.17G [00:02<00:07, 137MB/s]"},"key":"ACsIT3d2LZ"},{"type":"output","children":[],"jupyter_data":{"output_type":"stream","name":"stdout","text":"\rcheckpoints/uma-s-1.pt:  20%|███▏            | 235M/1.17G [00:02<00:04, 204MB/s]"},"key":"pg0TtHTP0L"},{"type":"output","children":[],"jupyter_data":{"output_type":"stream","name":"stdout","text":"\rcheckpoints/uma-s-1.pt:  31%|█████           | 370M/1.17G [00:02<00:02, 338MB/s]"},"key":"niiXAZLtd6"},{"type":"output","children":[],"jupyter_data":{"output_type":"stream","name":"stdout","text":"\rcheckpoints/uma-s-1.pt:  43%|██████▊         | 503M/1.17G [00:02<00:01, 480MB/s]"},"key":"RHHIWcXeLc"},{"type":"output","children":[],"jupyter_data":{"output_type":"stream","name":"stdout","text":"\rcheckpoints/uma-s-1.pt:  54%|████████▋       | 637M/1.17G [00:02<00:00, 574MB/s]"},"key":"NdxiJPtQuc"},{"type":"output","children":[],"jupyter_data":{"output_type":"stream","name":"stdout","text":"\rcheckpoints/uma-s-1.pt:  66%|██████████▌     | 771M/1.17G [00:02<00:00, 671MB/s]"},"key":"OrWoRkhBxk"},{"type":"output","children":[],"jupyter_data":{"output_type":"stream","name":"stdout","text":"\rcheckpoints/uma-s-1.pt:  77%|████████████▎   | 905M/1.17G [00:02<00:00, 799MB/s]"},"key":"u3KnzrrngW"},{"type":"output","children":[],"jupyter_data":{"output_type":"stream","name":"stdout","text":"\rcheckpoints/uma-s-1.pt:  89%|█████████████▎ | 1.04G/1.17G [00:03<00:00, 910MB/s]"},"key":"FvKLpBmSUW"},{"type":"output","children":[],"jupyter_data":{"output_type":"stream","name":"stdout","text":"\rcheckpoints/uma-s-1.pt: 100%|██████████████| 1.17G/1.17G [00:03<00:00, 1.01GB/s]\rcheckpoints/uma-s-1.pt: 100%|███████████████| 1.17G/1.17G [00:03<00:00, 374MB/s]\r\n"},"key":"C0dE6uDSAN"},{"type":"output","children":[],"jupyter_data":{"output_type":"stream","name":"stdout","text":"WARNING:root:initialize_finetuning_model starting from checkpoint_location: /home/runner/.cache/fairchem/models--facebook--UMA/snapshots/38529caa2c51a9a8a0d71f0b56b79ac33bc9eceb/checkpoints/uma-s-1.pt\r\n"},"key":"ZOV3ptFWab"},{"type":"output","children":[],"jupyter_data":{"output_type":"stream","name":"stdout","text":"INFO:root:Train Dataloader size 1\r\nINFO:root:Eval Dataloader size 1\r\nINFO:root:No existing checkpoints found, starting from scratch\r\nINFO:torchtnt.framework.fit:Started fit with max_epochs=2 max_steps=None max_train_steps_per_epoch=None max_eval_steps_per_epoch=None evaluate_every_n_steps=100 evaluate_every_n_epochs=1 \r\nINFO:torchtnt.framework.train:Started train with max_epochs=2, max_steps=None, max_steps_per_epoch=None\r\n"},"key":"Yglk4VGSpj"},{"type":"output","children":[],"jupyter_data":{"output_type":"stream","name":"stdout","text":"INFO:root:on_train_start: setting sampler state to 0, 0\r\nINFO:root:at beginning of epoch 0, setting sampler start step to 0\r\nINFO:torchtnt.framework.train:Started train epoch\r\nINFO:root:at beginning of epoch 0, setting sampler start step to 0\r\n\rTrain Epoch 0:   0%|                                     | 0/1 [00:00<?, ?it/s]\r\n"},"key":"H6LgjVViSY"},{"type":"output","children":[],"jupyter_data":{"output_type":"stream","name":"stdout","text":"[rank0]: Traceback (most recent call last):\r\n[rank0]:   File \"/home/runner/work/_tool/Python/3.12.12/x64/bin/fairchem\", line 6, in <module>\r\n[rank0]:     sys.exit(main())\r\n[rank0]:              ^^^^^^\r\n[rank0]:   File \"/home/runner/work/_tool/Python/3.12.12/x64/lib/python3.12/site-packages/fairchem/core/_cli.py\", line 137, in main\r\n[rank0]:     local_launch(cfg, log_dir)\r\n[rank0]:   File \"/home/runner/work/_tool/Python/3.12.12/x64/lib/python3.12/site-packages/fairchem/core/launchers/slurm_launch.py\", line 326, in local_launch\r\n[rank0]:     runner_wrapper(cfg)\r\n[rank0]:   File \"/home/runner/work/_tool/Python/3.12.12/x64/lib/python3.12/site-packages/fairchem/core/launchers/slurm_launch.py\", line 88, in runner_wrapper\r\n[rank0]:     SlurmSPMDProgram()(config, run_type)\r\n[rank0]:   File \"/home/runner/work/_tool/Python/3.12.12/x64/lib/python3.12/site-packages/fairchem/core/launchers/slurm_launch.py\", line 167, in __call__\r\n[rank0]:     self.runner.run()\r\n[rank0]:   File \"/home/runner/work/_tool/Python/3.12.12/x64/lib/python3.12/site-packages/fairchem/core/components/train/train_runner.py\", line 159, in run\r\n[rank0]:     fit(\r\n[rank0]:   File \"/home/runner/work/_tool/Python/3.12.12/x64/lib/python3.12/site-packages/torchtnt/framework/fit.py\", line 133, in fit\r\n[rank0]:     _train_impl(state, unit, callback_handler)\r\n[rank0]:   File \"/home/runner/work/_tool/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\r\n[rank0]:     return func(*args, **kwargs)\r\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/home/runner/work/_tool/Python/3.12.12/x64/lib/python3.12/site-packages/torchtnt/framework/train.py\", line 148, in _train_impl\r\n[rank0]:     _train_epoch_impl(state, train_unit, callback_handler)\r\n[rank0]:   File \"/home/runner/work/_tool/Python/3.12.12/x64/lib/python3.12/site-packages/torchtnt/framework/train.py\", line 214, in _train_epoch_impl\r\n[rank0]:     callback_handler.on_train_step_start(state, train_unit)\r\n[rank0]:   File \"/home/runner/work/_tool/Python/3.12.12/x64/lib/python3.12/site-packages/torchtnt/framework/_callback_handler.py\", line 140, in on_train_step_start\r\n[rank0]:     cb.on_train_step_start(state, unit)\r\n[rank0]:   File \"/home/runner/work/_tool/Python/3.12.12/x64/lib/python3.12/site-packages/fairchem/core/components/train/train_runner.py\", line 102, in on_train_step_start\r\n[rank0]:     self.save_callback(os.path.join(self.checkpoint_dir, f\"step_{step}\"))\r\n[rank0]:   File \"/home/runner/work/_tool/Python/3.12.12/x64/lib/python3.12/site-packages/fairchem/core/components/train/train_runner.py\", line 193, in save_state\r\n[rank0]:     self.train_eval_unit.save_state(checkpoint_location)\r\n[rank0]:   File \"/home/runner/work/_tool/Python/3.12.12/x64/lib/python3.12/site-packages/fairchem/core/units/mlip_unit/mlip_unit.py\", line 868, in save_state\r\n[rank0]:     dcp.save(state, checkpoint_id=checkpoint_location)\r\n[rank0]:   File \"/home/runner/work/_tool/Python/3.12.12/x64/lib/python3.12/site-packages/torch/distributed/checkpoint/logger.py\", line 87, in wrapper\r\n[rank0]:     result = func(*args, **kwargs)\r\n[rank0]:              ^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/home/runner/work/_tool/Python/3.12.12/x64/lib/python3.12/site-packages/torch/distributed/checkpoint/utils.py\", line 475, in inner_func\r\n[rank0]:     return func(*args, **kwargs)\r\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/home/runner/work/_tool/Python/3.12.12/x64/lib/python3.12/site-packages/torch/distributed/checkpoint/state_dict_saver.py\", line 176, in save\r\n[rank0]:     return _save_state_dict(\r\n[rank0]:            ^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/home/runner/work/_tool/Python/3.12.12/x64/lib/python3.12/site-packages/torch/distributed/checkpoint/state_dict_saver.py\", line 379, in _save_state_dict\r\n[rank0]:     return distW.all_reduce(\"write\", write_data, finish_checkpoint)\r\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/home/runner/work/_tool/Python/3.12.12/x64/lib/python3.12/site-packages/torch/distributed/checkpoint/utils.py\", line 259, in all_reduce\r\n[rank0]:     raise final_result\r\n[rank0]: torch.distributed.checkpoint.api.CheckpointException: CheckpointException ranks:dict_keys([0])\r\n[rank0]: Traceback (most recent call last): (RANK 0)\r\n[rank0]:   File \"/home/runner/work/_tool/Python/3.12.12/x64/lib/python3.12/site-packages/torch/distributed/checkpoint/utils.py\", line 239, in all_reduce\r\n[rank0]:     local_data = map_fun()\r\n[rank0]:                  ^^^^^^^^^\r\n[rank0]:   File \"/home/runner/work/_tool/Python/3.12.12/x64/lib/python3.12/site-packages/torch/distributed/checkpoint/logger.py\", line 87, in wrapper\r\n[rank0]:     result = func(*args, **kwargs)\r\n[rank0]:              ^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/home/runner/work/_tool/Python/3.12.12/x64/lib/python3.12/site-packages/torch/distributed/checkpoint/state_dict_saver.py\", line 368, in write_data\r\n[rank0]:     all_writes = storage_writer.write_data(final_local_plan, planner)\r\n[rank0]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/home/runner/work/_tool/Python/3.12.12/x64/lib/python3.12/site-packages/torch/distributed/checkpoint/filesystem.py\", line 677, in write_data\r\n[rank0]:     return self._write_data(planner, file_queue)\r\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/home/runner/work/_tool/Python/3.12.12/x64/lib/python3.12/site-packages/torch/distributed/checkpoint/filesystem.py\", line 705, in _write_data\r\n[rank0]:     _write_files_from_queue(\r\n[rank0]:   File \"/home/runner/work/_tool/Python/3.12.12/x64/lib/python3.12/site-packages/torch/distributed/checkpoint/filesystem.py\", line 428, in _write_files_from_queue\r\n[rank0]:     for tensor, write_item in loader.values():\r\n[rank0]:                               ^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/home/runner/work/_tool/Python/3.12.12/x64/lib/python3.12/site-packages/torch/distributed/checkpoint/filesystem.py\", line 223, in values\r\n[rank0]:     self._refill()\r\n[rank0]:   File \"/home/runner/work/_tool/Python/3.12.12/x64/lib/python3.12/site-packages/torch/distributed/checkpoint/filesystem.py\", line 184, in _refill\r\n[rank0]:     tensor = tensor.to(device=\"cpu\", non_blocking=True)\r\n[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: torch.AcceleratorError: CUDA error: invalid argument\r\n[rank0]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n[rank0]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n[rank0]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n\r\n\r\n"},"key":"ZQw1lBLE5o"},{"type":"output","children":[],"jupyter_data":{"output_type":"stream","name":"stdout","text":"\rTrain Epoch 0:   0%|                                     | 0/1 [00:07<?, ?it/s]\r\n\r\n"},"key":"gFdk5rc997"},{"type":"output","children":[],"jupyter_data":{"output_type":"stream","name":"stdout","text":"[rank0]:[W217 20:59:05.681544928 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\r\n"},"key":"UEOYj52OYL"}],"key":"KKUA4CjyuA"}],"key":"PdEYpU0gDj"},{"type":"block","children":[{"type":"paragraph","position":{"start":{"line":90,"column":1},"end":{"line":90,"column":1}},"children":[{"type":"text","value":"The basic YAML configuration looks like the following:","position":{"start":{"line":90,"column":1},"end":{"line":90,"column":1}},"key":"fEx0IGZV4w"}],"key":"I2weImMAuV"},{"type":"code","lang":"yaml","value":"job:\n  device_type: CUDA\n  scheduler:\n    mode: LOCAL\n    ranks_per_node: 1\n    num_nodes: 1\n  debug: True\n  run_dir: /tmp/uma_finetune_runs/\n  run_name: uma_finetune\n  logger:\n    _target_: fairchem.core.common.logger.WandBSingletonLogger.init_wandb\n    _partial_: true\n    entity: example\n    project: uma_finetune\n\n\nbase_model_name: uma-s-1p1\nmax_neighbors: 300\nepochs: 1\nsteps: null\nbatch_size: 2\nlr: 4e-4\n\ntrain_dataloader ...\neval_dataloader ...\nrunner ...","position":{"start":{"line":92,"column":1},"end":{"line":119,"column":1}},"key":"OrRvLydQVu"},{"type":"admonition","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"Configuration Parameters","position":{"start":{"line":121,"column":1},"end":{"line":121,"column":1}},"key":"qdXFJmkxr3"}],"key":"TceUI58XoV"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":124,"column":1},"end":{"line":138,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":124,"column":1},"end":{"line":131,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":124,"column":1},"end":{"line":124,"column":1}},"children":[{"type":"strong","position":{"start":{"line":124,"column":1},"end":{"line":124,"column":1}},"children":[{"type":"text","value":"base_model_name","position":{"start":{"line":124,"column":1},"end":{"line":124,"column":1}},"key":"xS9ovJLL6z"}],"key":"vezjC7mSfL"},{"type":"text","value":": Refers to a model name that can be retrieved from ","position":{"start":{"line":124,"column":1},"end":{"line":124,"column":1}},"key":"wPbE8dmToP"},{"type":"link","url":"https://huggingface.co/facebook/UMA","position":{"start":{"line":124,"column":1},"end":{"line":124,"column":1}},"children":[{"type":"text","value":"HuggingFace","position":{"start":{"line":124,"column":1},"end":{"line":124,"column":1}},"key":"cTUoShTZrF"}],"urlSource":"https://huggingface.co/facebook/UMA","key":"bb3xfuioxw"},{"type":"text","value":". If you want to use your custom UMA checkpoint, provide the path directly in the runner:","position":{"start":{"line":124,"column":1},"end":{"line":124,"column":1}},"key":"XKhhFqRIAO"}],"key":"pleJDJRwrN"},{"type":"code","lang":"yaml","value":"model:\n  _target_: fairchem.core.units.mlip_unit.mlip_unit.initialize_finetuning_model\n  checkpoint_location: /path/to/your/checkpoint.pt","position":{"start":{"line":126,"column":1},"end":{"line":130,"column":1}},"key":"mdbnON7DO2"}],"key":"KkSVOuyUhA"},{"type":"listItem","spread":true,"position":{"start":{"line":132,"column":1},"end":{"line":133,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":132,"column":1},"end":{"line":132,"column":1}},"children":[{"type":"strong","position":{"start":{"line":132,"column":1},"end":{"line":132,"column":1}},"children":[{"type":"text","value":"max_neighbors","position":{"start":{"line":132,"column":1},"end":{"line":132,"column":1}},"key":"ut4hocdQTS"}],"key":"XQOuaSe470"},{"type":"text","value":": The number of neighbors used for the equivariant SO2 convolutions. 300 is the default used in UMA training, but if you don’t have a lot of memory, 100 is usually fine to ensure smoothness of the potential (see the ","position":{"start":{"line":132,"column":1},"end":{"line":132,"column":1}},"key":"Bmus7UoQul"},{"type":"link","url":"https://arxiv.org/abs/2502.12147","position":{"start":{"line":132,"column":1},"end":{"line":132,"column":1}},"children":[{"type":"text","value":"ESEN paper","position":{"start":{"line":132,"column":1},"end":{"line":132,"column":1}},"key":"u31f2p66kV"}],"urlSource":"https://arxiv.org/abs/2502.12147","key":"gHVOncIeT3"},{"type":"text","value":").","position":{"start":{"line":132,"column":1},"end":{"line":132,"column":1}},"key":"klebN4qfe6"}],"key":"WFWHihgFXy"}],"key":"jtGqrHPRVS"},{"type":"listItem","spread":true,"position":{"start":{"line":134,"column":1},"end":{"line":135,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":134,"column":1},"end":{"line":134,"column":1}},"children":[{"type":"strong","position":{"start":{"line":134,"column":1},"end":{"line":134,"column":1}},"children":[{"type":"text","value":"epochs","position":{"start":{"line":134,"column":1},"end":{"line":134,"column":1}},"key":"OTWWGaYTSr"}],"key":"L9w9vP5Lc2"},{"type":"text","value":", ","position":{"start":{"line":134,"column":1},"end":{"line":134,"column":1}},"key":"q0jwcYuQFx"},{"type":"strong","position":{"start":{"line":134,"column":1},"end":{"line":134,"column":1}},"children":[{"type":"text","value":"steps","position":{"start":{"line":134,"column":1},"end":{"line":134,"column":1}},"key":"HoFLERlo1h"}],"key":"Bb6hbzsIxG"},{"type":"text","value":": Choose to either run for an integer number of epochs or steps. Only 1 can be specified; the other must be null.","position":{"start":{"line":134,"column":1},"end":{"line":134,"column":1}},"key":"xvQdfJ0ptD"}],"key":"ZDkI6g5Euk"}],"key":"XNev9lJWFQ"},{"type":"listItem","spread":true,"position":{"start":{"line":136,"column":1},"end":{"line":137,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":136,"column":1},"end":{"line":136,"column":1}},"children":[{"type":"strong","position":{"start":{"line":136,"column":1},"end":{"line":136,"column":1}},"children":[{"type":"text","value":"batch_size","position":{"start":{"line":136,"column":1},"end":{"line":136,"column":1}},"key":"QgANv9TKrl"}],"key":"qOR63mA5NC"},{"type":"text","value":": In this configuration we use the batch sampler. Start with the largest batch size that can fit on your system without running out of memory. However, don’t use a batch size so large that you complete training in very few steps. The optimal batch size is usually the one that minimizes the final validation loss for a fixed compute budget.","position":{"start":{"line":136,"column":1},"end":{"line":136,"column":1}},"key":"thBbTfsGVq"}],"key":"ZUMMFnIYeR"}],"key":"d1tZXtvWok"},{"type":"listItem","spread":true,"position":{"start":{"line":138,"column":1},"end":{"line":138,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":138,"column":1},"end":{"line":138,"column":1}},"children":[{"type":"strong","position":{"start":{"line":138,"column":1},"end":{"line":138,"column":1}},"children":[{"type":"text","value":"lr","position":{"start":{"line":138,"column":1},"end":{"line":138,"column":1}},"key":"nbIHrCRDSr"}],"key":"oN4j5oT33G"},{"type":"text","value":", ","position":{"start":{"line":138,"column":1},"end":{"line":138,"column":1}},"key":"o3QORAsq3o"},{"type":"strong","position":{"start":{"line":138,"column":1},"end":{"line":138,"column":1}},"children":[{"type":"text","value":"weight_decay","position":{"start":{"line":138,"column":1},"end":{"line":138,"column":1}},"key":"RivxfznFNv"}],"key":"sDN6cJ0sqU"},{"type":"text","value":": These are standard learning parameters. The recommended values we use are the defaults.","position":{"start":{"line":138,"column":1},"end":{"line":138,"column":1}},"key":"cgd4BKlJXr"}],"key":"IuGyRLMXJj"}],"key":"YnPEkSdFKd"}],"key":"runMMNjcro"}],"class":"dropdown","key":"lgy4oeUIso"},{"type":"heading","depth":3,"position":{"start":{"line":141,"column":1},"end":{"line":141,"column":1}},"children":[{"type":"text","value":"Logging and Artifacts","position":{"start":{"line":141,"column":1},"end":{"line":141,"column":1}},"key":"NtWyWEnzdD"}],"identifier":"logging-and-artifacts","label":"Logging and Artifacts","html_id":"logging-and-artifacts","implicit":true,"key":"N8iEnnxsDA"},{"type":"paragraph","position":{"start":{"line":143,"column":1},"end":{"line":143,"column":1}},"children":[{"type":"text","value":"For logging and checkpoints, all artifacts are stored in the location specified in ","position":{"start":{"line":143,"column":1},"end":{"line":143,"column":1}},"key":"oPVUAusdIB"},{"type":"inlineCode","value":"job.run_dir","position":{"start":{"line":143,"column":1},"end":{"line":143,"column":1}},"key":"J7gr1op92e"},{"type":"text","value":". The visual logger we support is ","position":{"start":{"line":143,"column":1},"end":{"line":143,"column":1}},"key":"R39BlIxLtd"},{"type":"link","url":"https://wandb.ai/site/","position":{"start":{"line":143,"column":1},"end":{"line":143,"column":1}},"children":[{"type":"text","value":"Weights and Biases","position":{"start":{"line":143,"column":1},"end":{"line":143,"column":1}},"key":"Z7sIW6bUhS"}],"urlSource":"https://wandb.ai/site/","key":"Qjf2gYJvB8"},{"type":"text","value":".","position":{"start":{"line":143,"column":1},"end":{"line":143,"column":1}},"key":"Esz4T6sOKb"}],"key":"auxbXNkw0Z"},{"type":"admonition","kind":"warning","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"Warning","key":"yhEAUAy2e1"}],"key":"Nww9OULtFU"},{"type":"paragraph","position":{"start":{"line":146,"column":1},"end":{"line":146,"column":1}},"children":[{"type":"text","value":"Tensorboard is no longer supported. You must set up your W&B account separately and ","position":{"start":{"line":146,"column":1},"end":{"line":146,"column":1}},"key":"pku4KQZFMp"},{"type":"inlineCode","value":"job.debug","position":{"start":{"line":146,"column":1},"end":{"line":146,"column":1}},"key":"nICthdMI0G"},{"type":"text","value":" must be set to ","position":{"start":{"line":146,"column":1},"end":{"line":146,"column":1}},"key":"tRbeafqcqd"},{"type":"inlineCode","value":"False","position":{"start":{"line":146,"column":1},"end":{"line":146,"column":1}},"key":"VOnDJisslT"},{"type":"text","value":" for W&B logging to work.","position":{"start":{"line":146,"column":1},"end":{"line":146,"column":1}},"key":"HttA5N7qoG"}],"key":"ICmmYmb3Ky"}],"key":"lydk5N4uf9"},{"type":"heading","depth":3,"position":{"start":{"line":149,"column":1},"end":{"line":149,"column":1}},"children":[{"type":"text","value":"Distributed Training","position":{"start":{"line":149,"column":1},"end":{"line":149,"column":1}},"key":"crhH3Z20WF"}],"identifier":"distributed-training","label":"Distributed Training","html_id":"distributed-training","implicit":true,"key":"tbTCEI7x1c"},{"type":"paragraph","position":{"start":{"line":151,"column":1},"end":{"line":151,"column":1}},"children":[{"type":"text","value":"We support multi-GPU distributed training without additional infrastructure and multi-node distributed training on ","position":{"start":{"line":151,"column":1},"end":{"line":151,"column":1}},"key":"lhQfPQDeW0"},{"type":"link","url":"https://slurm.schedmd.com/documentation.html","position":{"start":{"line":151,"column":1},"end":{"line":151,"column":1}},"children":[{"type":"text","value":"SLURM","position":{"start":{"line":151,"column":1},"end":{"line":151,"column":1}},"key":"ak4a2A0PYH"}],"urlSource":"https://slurm.schedmd.com/documentation.html","key":"rdLaBj5mXU"},{"type":"text","value":" only.","position":{"start":{"line":151,"column":1},"end":{"line":151,"column":1}},"key":"tzzxPp5402"}],"key":"Srp9ivmtTR"},{"type":"paragraph","position":{"start":{"line":153,"column":1},"end":{"line":153,"column":1}},"children":[{"type":"strong","position":{"start":{"line":153,"column":1},"end":{"line":153,"column":1}},"children":[{"type":"text","value":"Multi-GPU locally:","position":{"start":{"line":153,"column":1},"end":{"line":153,"column":1}},"key":"z83gb5IySx"}],"key":"Oz8LbGNPrM"},{"type":"text","value":" Simply set ","position":{"start":{"line":153,"column":1},"end":{"line":153,"column":1}},"key":"T4Gu31WeaO"},{"type":"inlineCode","value":"job.scheduler.ranks_per_node=N","position":{"start":{"line":153,"column":1},"end":{"line":153,"column":1}},"key":"ahWSItokjP"},{"type":"text","value":" where N is the number of GPUs you want to train on.","position":{"start":{"line":153,"column":1},"end":{"line":153,"column":1}},"key":"pqbiaHKcGd"}],"key":"qpk7hp5kh4"},{"type":"paragraph","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"children":[{"type":"strong","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"children":[{"type":"text","value":"Multi-node on SLURM:","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"capDfng7bv"}],"key":"azyrEjxHM0"},{"type":"text","value":" Change ","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"alzXvFPuU1"},{"type":"inlineCode","value":"job.scheduler.mode=SLURM","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"RivC5J61rW"},{"type":"text","value":" and set both ","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"Q4jXIKL1Qc"},{"type":"inlineCode","value":"job.scheduler.ranks_per_node","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"lbFszfae3Y"},{"type":"text","value":" and ","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"GnZamPj8Io"},{"type":"inlineCode","value":"job.scheduler.num_nodes","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"TTg94DRdJ6"},{"type":"text","value":" to the desired values.","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"CacqPMVZVw"}],"key":"TaCwDnhEkc"},{"type":"admonition","kind":"note","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"Note","key":"SeJZxZNjsM"}],"key":"Ud0y7FA8fa"},{"type":"paragraph","position":{"start":{"line":158,"column":1},"end":{"line":158,"column":1}},"children":[{"type":"text","value":"The ","position":{"start":{"line":158,"column":1},"end":{"line":158,"column":1}},"key":"u0nDEj673U"},{"type":"inlineCode","value":"run_dir","position":{"start":{"line":158,"column":1},"end":{"line":158,"column":1}},"key":"dqlTARsXUv"},{"type":"text","value":" must be in a shared network accessible mount for multi-node training to work.","position":{"start":{"line":158,"column":1},"end":{"line":158,"column":1}},"key":"dHuC6yjIWb"}],"key":"YZ1dhYtIKg"}],"key":"w6JdxAQ8ke"},{"type":"heading","depth":3,"position":{"start":{"line":161,"column":1},"end":{"line":161,"column":1}},"children":[{"type":"text","value":"Resuming Runs","position":{"start":{"line":161,"column":1},"end":{"line":161,"column":1}},"key":"nGEsSKTpTk"}],"identifier":"resuming-runs","label":"Resuming Runs","html_id":"resuming-runs","implicit":true,"key":"WCPj2kyDvA"},{"type":"paragraph","position":{"start":{"line":163,"column":1},"end":{"line":163,"column":1}},"children":[{"type":"text","value":"To resume from a checkpoint in the middle of a run, find the checkpoint folder at the step you want and use the same fairchem command:","position":{"start":{"line":163,"column":1},"end":{"line":163,"column":1}},"key":"VYeggIpEXK"}],"key":"yjzSkBKnAR"}],"key":"axvNhwSxHE"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"! fairchem -c /tmp/finetune_dir/some_id/checkpoints/final/resume.yaml","visibility":"show","key":"GknQzNLMqn"},{"type":"outputs","id":"OukCf8HsnXexB75dm8xBL","children":[],"visibility":"show","key":"zQ2X9Ta7qx"}],"data":{"tags":["skip-execution"]},"visibility":"show","key":"Lwujt0AYjS"},{"type":"block","children":[{"type":"heading","depth":3,"position":{"start":{"line":170,"column":1},"end":{"line":170,"column":1}},"children":[{"type":"text","value":"Running Inference on the Fine-tuned Model","position":{"start":{"line":170,"column":1},"end":{"line":170,"column":1}},"key":"Ln3ZNkeqf0"}],"identifier":"running-inference-on-the-fine-tuned-model","label":"Running Inference on the Fine-tuned Model","html_id":"running-inference-on-the-fine-tuned-model","implicit":true,"key":"RhAzubBXwQ"},{"type":"paragraph","position":{"start":{"line":172,"column":1},"end":{"line":172,"column":1}},"children":[{"type":"text","value":"Inference is run in the same way as the UMA models, except you need to load the checkpoint from a local path.","position":{"start":{"line":172,"column":1},"end":{"line":172,"column":1}},"key":"bAeYyt1hvk"}],"key":"uh2MSUTSN6"},{"type":"admonition","kind":"warning","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"Warning","key":"OzpObwP18x"}],"key":"os1YpHEaUF"},{"type":"paragraph","position":{"start":{"line":175,"column":1},"end":{"line":175,"column":1}},"children":[{"type":"text","value":"You must use the same task that you used for fine-tuning!","position":{"start":{"line":175,"column":1},"end":{"line":175,"column":1}},"key":"i4GLEbalRV"}],"key":"lpzxhRPTr3"}],"key":"jZkMpWfZwQ"}],"key":"ZjA5MrDoqM"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"from fairchem.core.units.mlip_unit import load_predict_unit\nfrom fairchem.core import FAIRChemCalculator\n\npredictor = load_predict_unit(\"/tmp/finetune_dir/some_id/checkpoints/final/inference_ckpt.pt\")\ncalc = FAIRChemCalculator(predictor, task_name=\"omat\")","key":"y7j5bjxwgf"},{"type":"outputs","id":"HMd3REqbJ_0JUocS1kEOy","children":[{"type":"output","children":[],"jupyter_data":{"output_type":"stream","name":"stderr","text":"WARNING:root:device was not explicitly set, using device='cuda'.\n"},"key":"ApywdrMYMA"},{"type":"output","children":[],"jupyter_data":{"output_type":"error","traceback":"\u001b[31m---------------------------------------------------------------------------\u001b[39m\n\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)\n\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfairchem\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01munits\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmlip_unit\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_predict_unit\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfairchem\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FAIRChemCalculator\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m predictor = \u001b[43mload_predict_unit\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/tmp/finetune_dir/some_id/checkpoints/final/inference_ckpt.pt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m calc = FAIRChemCalculator(predictor, task_name=\u001b[33m\"\u001b[39m\u001b[33momat\u001b[39m\u001b[33m\"\u001b[39m)\n\n\u001b[36mFile \u001b[39m\u001b[32m~/work/_tool/Python/3.12.12/x64/lib/python3.12/site-packages/fairchem/core/units/mlip_unit/__init__.py:70\u001b[39m, in \u001b[36mload_predict_unit\u001b[39m\u001b[34m(path, inference_settings, overrides, device, atom_refs, form_elem_refs, workers)\u001b[39m\n\u001b[32m     60\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ParallelMLIPPredictUnit(\n\u001b[32m     61\u001b[39m         path,\n\u001b[32m     62\u001b[39m         device=device,\n\u001b[32m   (...)\u001b[39m\u001b[32m     67\u001b[39m         num_workers=workers,\n\u001b[32m     68\u001b[39m     )\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMLIPPredictUnit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m        \u001b[49m\u001b[43minference_settings\u001b[49m\u001b[43m=\u001b[49m\u001b[43minference_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m        \u001b[49m\u001b[43moverrides\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m        \u001b[49m\u001b[43matom_refs\u001b[49m\u001b[43m=\u001b[49m\u001b[43matom_refs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m        \u001b[49m\u001b[43mform_elem_refs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mform_elem_refs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\n\u001b[36mFile \u001b[39m\u001b[32m~/work/_tool/Python/3.12.12/x64/lib/python3.12/site-packages/fairchem/core/units/mlip_unit/predict.py:132\u001b[39m, in \u001b[36mMLIPPredictUnit.__init__\u001b[39m\u001b[34m(self, inference_model_path, device, overrides, inference_settings, seed, atom_refs, form_elem_refs, assert_on_nans)\u001b[39m\n\u001b[32m    127\u001b[39m     logging.warning(\n\u001b[32m    128\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe wigner_cuda flag is deprecated and will be removed in future versions.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    129\u001b[39m     )\n\u001b[32m    131\u001b[39m \u001b[38;5;66;03m# Load checkpoint first to get model type\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m checkpoint = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m    \u001b[49m\u001b[43minference_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m    134\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[38;5;66;03m# Build model-specific overrides\u001b[39;00m\n\u001b[32m    137\u001b[39m final_overrides = \u001b[38;5;28mself\u001b[39m._build_overrides_from_settings(\n\u001b[32m    138\u001b[39m     checkpoint, overrides, inference_settings\n\u001b[32m    139\u001b[39m )\n\n\u001b[36mFile \u001b[39m\u001b[32m~/work/_tool/Python/3.12.12/x64/lib/python3.12/site-packages/torch/serialization.py:1484\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1481\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args.keys():\n\u001b[32m   1482\u001b[39m     pickle_load_args[\u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1484\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[32m   1485\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[32m   1486\u001b[39m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[32m   1487\u001b[39m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[32m   1488\u001b[39m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[32m   1489\u001b[39m         orig_position = opened_file.tell()\n\n\u001b[36mFile \u001b[39m\u001b[32m~/work/_tool/Python/3.12.12/x64/lib/python3.12/site-packages/torch/serialization.py:759\u001b[39m, in \u001b[36m_open_file_like\u001b[39m\u001b[34m(name_or_buffer, mode)\u001b[39m\n\u001b[32m    757\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_open_file_like\u001b[39m(name_or_buffer: FileLike, mode: \u001b[38;5;28mstr\u001b[39m) -> _opener[IO[\u001b[38;5;28mbytes\u001b[39m]]:\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[32m--> \u001b[39m\u001b[32m759\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    760\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    761\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n\n\u001b[36mFile \u001b[39m\u001b[32m~/work/_tool/Python/3.12.12/x64/lib/python3.12/site-packages/torch/serialization.py:740\u001b[39m, in \u001b[36m_open_file.__init__\u001b[39m\u001b[34m(self, name, mode)\u001b[39m\n\u001b[32m    739\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike[\u001b[38;5;28mstr\u001b[39m]], mode: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m740\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n\n\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/tmp/finetune_dir/some_id/checkpoints/final/inference_ckpt.pt'","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/tmp/finetune_dir/some_id/checkpoints/final/inference_ckpt.pt'"},"key":"Oli2Pwtirb"}],"key":"wf8zilB6Ij"}],"key":"PUyhh8sVUp"}],"key":"LCs90IIZFc"},"references":{"cite":{"order":[],"data":{}}},"footer":{"navigation":{"prev":{"title":"Evaluating Pretrained Models","url":"/evaluation","group":"AI/ML Models & Usage"},"next":{"title":"Calculation Workflows with FAIRChem Models","url":"/workflows","group":"AI/ML Models & Usage"}}},"domain":"http://localhost:3000"}